name: Python runcode example
help: |
  Provides an example of the pipes inline python command. This example 
  doesn't write to a database and uses the local filesystem only.

  Flows:
  ======
    default

      Use the inline python command and generate the file 
      zabbix_piped_through_python.json in your current working directory.

    --flows data

      Use --flows data to generate the example dataset. This will create the 
      file {{ vars.output_file }} in your current working directory.

    --flows clean

      Use --flows clean to remove the generated files.
driver: local
cluster: http://localhost
shard: http://localhost
workflow:
  - data
  - clean
  - input:
      executor: UnixPipeExecutor
      enabled: true
vars:
  output_file: zabbix_data_test.csv.gz
data:
  - tasks.file.write:
      path: zabbix_data_test.csv
      data: |
        "somekey1":"somevalue1";"somekey2":"somevalue2";"somekey3":"somevalue3"
        "somekey11":"somevalue12";"somekey22":"somevalue22";"somekey32":"somevalue32"
        "somekey12":"somevalue13";"somekey23":"somevalue23";"somekey33":"somevalue33"
  - tasks.pack.gz:
      input_file: zabbix_data_test.csv
      output_file: {{ vars.output_file }}
clean:
  - tasks.file.delete:
    - {{ vars.output_file }}
    - zabbix_piped_through_python.json
input:
  - io.file.read: {{ vars.output_file }}
  - pipes.unpack.unpack
  - pipes.python.runcode:
      python: python3.7
      run: |
        import json
        import sys

        for line in sys.stdin:
            data = dict(v.replace('"', '').split(':')[0:2] for v in line.strip().split(';'))
            sys.stdout.write(f'{{ vars.output_file }}: {json.dumps(data)}\n')
  - io.file.write: zabbix_piped_through_python.json
